import pandas as pd
import json
import time
import os
from tqdm import tqdm
from dotenv import load_dotenv
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import re

load_dotenv()

REQUEST_DELAY = 0.5
MAX_RETRIES = 3

MODEL_NAME = "VietAI/envit5-translation"
device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Loading model: {MODEL_NAME}")
print(f"Device: {device}")

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)

print("Model loaded successfully!\n")


def translate_single(dialogue: str) -> str:
    """
    D·ªãch dialogue - t·ª± ƒë·ªông chia nh·ªè n·∫øu qu√° d√†i
    """
    try:
        # Ki·ªÉm tra ƒë·ªô d√†i
        test_tokens = tokenizer.encode(f"en: {dialogue}", add_special_tokens=True)
        
        # N·∫øu qu√° d√†i (>400 tokens input), chia nh·ªè theo c√¢u
        if len(test_tokens) > 400:
            return translate_long_dialogue(dialogue)
        
        # N·∫øu ng·∫Øn, d·ªãch b√¨nh th∆∞·ªùng
        input_text = f"en: {dialogue}"
        
        inputs = tokenizer(
            input_text,
            return_tensors="pt",
            padding=True,
            max_length=512,
            truncation=True
        ).input_ids.to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=1024,  # TƒÇNG l√™n 1024 cho output
                num_beams=5,
                early_stopping=True
            )
        
        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
        translation = post_process_translation(translation)
        
        return translation.strip()
        
    except Exception as e:
        raise e


def translate_long_dialogue(dialogue: str) -> str:
    """
    D·ªãch dialogue d√†i b·∫±ng c√°ch chia nh·ªè theo t·ª´ng l∆∞·ª£t h·ªôi tho·∫°i
    """
    # T√°ch theo pattern "Innocent:" v√† "Suspect:"
    parts = re.split(r'(Innocent:|Suspect:)', dialogue)
    
    translated_parts = []
    current_speaker = None
    
    for part in parts:
        part = part.strip()
        if not part:
            continue
            
        # N·∫øu l√† label speaker
        if part in ['Innocent:', 'Suspect:']:
            current_speaker = part
            continue
        
        # D·ªãch t·ª´ng ƒëo·∫°n text
        if current_speaker:
            input_text = f"en: {part}"
            
            inputs = tokenizer(
                input_text,
                return_tensors="pt",
                padding=True,
                max_length=512,
                truncation=True
            ).input_ids.to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=1024,
                    num_beams=5,
                    early_stopping=True
                )
            
            translation = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Gh√©p v·ªõi speaker label
            translated_parts.append(f"{current_speaker} {translation}")
            current_speaker = None
    
    # Gh√©p t·∫•t c·∫£ l·∫°i
    full_translation = ' '.join(translated_parts)
    full_translation = post_process_translation(full_translation)
    
    return full_translation.strip()


def post_process_translation(translation: str) -> str:
    
    translation = re.sub(r'^vi:\s*', '', translation, flags=re.IGNORECASE)
    translation = re.sub(r'\nvi:\s*', '\n', translation, flags=re.IGNORECASE)
    
    label_replacements = [
        ("V√¥ t·ªôi:", "Ng∆∞·ªùi nh·∫≠n cu·ªôc g·ªçi:"),
        ("Nghi ng·ªù:", "K·∫ª l·ª´a ƒë·∫£o:"),
        ("Nghi ph·∫°m:", "K·∫ª l·ª´a ƒë·∫£o:"),
        ("Nghi can:", "K·∫ª l·ª´a ƒë·∫£o:"),
        ("Ng∆∞·ªùi b·ªã nghi ng·ªù:", "K·∫ª l·ª´a ƒë·∫£o:"),
        ("Innocent:", "Ng∆∞·ªùi nh·∫≠n cu·ªôc g·ªçi:"),
        ("Suspect:", "K·∫ª l·ª´a ƒë·∫£o:"),
    ]
    
    for old, new in label_replacements:
        translation = translation.replace(old, new)
    
    replacements = {
        r"C·ª•c An sinh x√£ h·ªôi": "B·ªô C√¥ng an",
        r"C∆° quan An sinh X√£ h·ªôi": "B·ªô C√¥ng an",
        r"B·∫£o hi·ªÉm X√£ h·ªôi": "B·ªô C√¥ng an",
        r"Social Security Administration": "B·ªô C√¥ng an",
        r"·ª¶y ban Th∆∞∆°ng m·∫°i Li√™n bang": "·ª¶y ban Th∆∞∆°ng m·∫°i",
        r"Federal Trade Commission": "·ª¶y ban Th∆∞∆°ng m·∫°i",
        r"D·ªãch v·ª• T√≠n d·ª•ng Allied": "D·ªãch v·ª• T√≠n d·ª•ng Allied",
    
        r"s·ªë an sinh x√£ h·ªôi": "s·ªë CCCD",
        r"social security number": "s·ªë CCCD",
        r"\bSSN\b": "CCCD",

        r"Sƒ© quan": "C√°n b·ªô",
        r"Officer": "C√°n b·ªô",
        r"Thanh tra vi√™n": "Thanh tra",
        
        r"th∆∞a b√†": "ch·ªã",
        r"th∆∞a √¥ng": "anh",
        r"ma'am": "ch·ªã",
        r"sir": "anh",
        r"\bc√¥\b(?! ·∫•y)": "ch·ªã", 
    }
    
    for pattern, replacement in replacements.items():
        translation = re.sub(pattern, replacement, translation, flags=re.IGNORECASE)
 
    translation = re.sub(
        r'(?<!^)(\s*)(Ng∆∞·ªùi nh·∫≠n cu·ªôc g·ªçi:|K·∫ª l·ª´a ƒë·∫£o:)', 
        r'\n\2', 
        translation
    )
    
    translation = re.sub(r'\n\s*\n+', '\n', translation) 
    translation = re.sub(r' +', ' ', translation)          
    translation = re.sub(r'^\s+', '', translation)         
    
    return translation.strip()


def load_progress(path):
    if not os.path.exists(path):
        return []
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            print(f"üìÇ Resume: {len(data)} d√≤ng ƒë√£ d·ªãch")
            return data
    except:
        return []


def save_one(path, record):
    """L∆∞u t·ª´ng k·∫øt qu·∫£ v√†o file JSON"""
    data = load_progress(path)
    data.append(record)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def process_csv(csv_file, output_json, test_mode=False):
    print("üéØ Model:", MODEL_NAME)
    print(f"üñ•Ô∏è Device: {device}")
    print("‚úÖ S·∫µn s√†ng d·ªãch\n")

    df = pd.read_csv(csv_file, encoding="utf-8-sig")
    print(f"üìñ CSV rows: {len(df)}\n")

    if test_mode:
        df = df.head(10)
        print("‚ö†Ô∏è TEST MODE (10 d√≤ng)\n")

    existing = load_progress(output_json)
    done_ids = {x["id"] for x in existing}

    to_translate = []
    errors = []

    for idx, row in df.iterrows():
        line_id = idx + 1

        if line_id in done_ids:
            continue

        dialogue = str(row.get("dialogue", "")).strip()

        if len(dialogue) < 50:
            errors.append({
                "line": line_id,
                "reason": "Dialogue qu√° ng·∫Øn"
            })
            continue

        to_translate.append({
            "id": line_id,
            "dialogue": dialogue,
            "personality": row.get("personality"),
            "type": row.get("type"),
            "labels": row.get("labels")
        })

    if not to_translate:
        print("‚úÖ T·∫§T C·∫¢ ƒê√É D·ªäCH XONG!")
        return existing, errors

    print(f"üöÄ C·∫ßn d·ªãch: {len(to_translate)} m·∫´u\n")

    success = 0

    for idx, item in enumerate(tqdm(to_translate, desc="Translating"), 1):
        retry = 0
        
        while retry < MAX_RETRIES:
            try:
                vi_text = translate_single(item["dialogue"])

                if len(vi_text) < 30:
                    raise Exception("Translation too short")

                result = {
                    "id": item["id"],
                    "dialogue_original": item["dialogue"],
                    "dialogue_vietnamese": vi_text,
                    "personality": item["personality"],
                    "type": item["type"],
                    "labels": item["labels"]
                }

                save_one(output_json, result)
                success += 1

                if idx == 1:
                    print("\n" + "="*70)
                    print("üß™ SAMPLE TRANSLATION:")
                    print("="*70)
                    print("üìå ORIGINAL:")
                    print(item["dialogue"][:500])
                    print("\nüìå VIETNAMESE:")
                    print(vi_text[:500])
                    print("="*70 + "\n")

                break  

            except Exception as e:
                retry += 1
                print(f"\n‚ùå L·ªói d√≤ng {item['id']}: {e}")
                
                if retry < MAX_RETRIES:
                    print(f"üîÑ Retry {retry}/{MAX_RETRIES}...")
                    time.sleep(2)
                else:
                    errors.append({
                        "line": item["id"],
                        "error": str(e),
                        "preview": item["dialogue"][:100]
                    })
                    break

        if idx < len(to_translate):
            time.sleep(REQUEST_DELAY)

    final = load_progress(output_json)

    if errors:
        error_file = output_json.replace('.json', '_errors.json')
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(errors, f, ensure_ascii=False, indent=2)
        print(f"‚ö†Ô∏è ƒê√£ l∆∞u errors v√†o: {error_file}")

    print("\n" + "="*70)
    print("üéâ HO√ÄN T·∫§T!")
    print(f"‚úÖ Th√†nh c√¥ng: {success}/{len(to_translate)} m·∫´u m·ªõi")
    print(f"üìä T·ªïng trong file: {len(final)} d√≤ng")
    print(f"‚ùå L·ªói: {len(errors)} d√≤ng")
    print(f"üíæ Output: {output_json}")
    print("="*70)

    return final, errors


if __name__ == "__main__":
    input_csv = r"C:\Users\admin\Desktop\Hoangvt\data_scam\raw\BothBosu\agent_conversation_all.csv"
    output_json = r"C:\Users\admin\Desktop\Hoangvt\data_scam\processed\agent_conversation_all.json"

    print("\n" + "="*70)
    print("CH·ªåN CH·∫æ ƒê·ªò:")
    print("1. TEST MODE - D·ªãch 10 d√≤ng ƒë·∫ßu")
    print("2. FULL MODE - D·ªãch t·∫•t c·∫£")
    print("="*70)
    
    mode = input("Ch·ªçn 1 ho·∫∑c 2: ").strip()

    process_csv(
        input_csv,
        output_json,
        test_mode=(mode == "1")
    )